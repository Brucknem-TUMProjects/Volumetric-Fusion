\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{media9}
\usepackage{filecontents}
%\usepackage{geometry}
\usepackage{caption}
\usepackage{subcaption}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Volumetric Capture}

\author{Marcel Bruckner\\
{\tt\small marcel.bruckner@tum.de}
\and
Kevin Bein\\
{\tt\small kevin.bein@tum.de}\\[0.5em]
Technische Universit\"at M\"unchen
\and
Moiz Sajid\\
{\tt\small moiz.sajid@tum.de}
}

\maketitle
%\thispagestyle{empty}

\newcommand{\intel}{Intel\textsuperscript{\textcopyright}}
\newcommand{\fref}[1]{\mbox{Figure~\ref{#1}}}
\newcommand{\tref}[1]{\mbox{Table~\ref{#1}}}
\newcommand{\sref}[1]{\mbox{Section~\ref{#1}}}
\newcommand{\rscamera}{\intel{} RealSense\texttrademark{} Depth Camera~D415 \cite{DepthCameraD415}}
\newcommand{\aruco}{ArUco~markers~\cite{ArUco}}
\newcommand{\charuco}{ChArUco~markers~\cite{ChArUco}}
\newcommand{\opencv}{OpenCV~\cite{OpenCV}}
\newcommand{\opengl}{OpenGL~\cite{OpenGL}}

%%%%%%%%% ABSTRACT
\begin{abstract}
   In this project, we implemented a real time mesh reconstruction of dynamic scenes using a multi view camera capture setup. We use three \rscamera{} for RGB-D data capturing which are calibrated using known correspondences from detected markers. The data is then fused into a voxelgrid to represent its implicit surface and following, a marching cubes algorithm is applied for the final surface extraction. Both qualitative and quantitative results are presented.   
\end{abstract}


\begin{figure}[t]
	\begin{center}
		\includegraphics[width=1.0\linewidth]{imgs/pipeline}
	\end{center}
	\caption{Reconstruction pipeline used in this project: RGB-D Frame Capture, Correspondence Finding, Camera Calibration, Voxelgrid TSDF, Marching Cubes}
	\label{fig:reconstruction-pipeline}
\end{figure}

%%%%%%%%% BODY TEXT
\section{Introduction}

Volumetric Capture is an extensively researched topic where the goal is to get an accurate 3D reconstruction of dynamic scenes in real time. The first work in the area by Cureless and Levoy~\cite{Authors4} integrated data from one range image into a cumulative weighted signed distance function. KinectFusion~\cite{Authors5} was another milestone in this field which proposed a real time mapping system of indoor scenes using a single depth camera. DynamicFusion~\cite{Authors6} handles non-rigid scenes and reconstruction over time. Our work focuses on reconstructing dynamic scenes on a per frame basis.\\ 
One possible application area of our project is Holoportation~\cite{Authors7} in which 3D reconstructed models are used for virtual reality interactions between two persons over huge distances.

\begin{figure}
	\centering
	\begin{subfigure}[t]{.315\linewidth}
		\centering\includegraphics[width=0.9\linewidth]{imgs/rgb}
		\caption{RGB Image}
		\label{fig:rgb}
	\end{subfigure}
	\begin{subfigure}[t]{.315\linewidth}
		\centering\includegraphics[width=0.9\linewidth]{imgs/depth}
		\caption{Depth Image}
		\label{fig:depth}
	\end{subfigure}
	\begin{subfigure}[t]{.315\linewidth}
		\centering\includegraphics[width=0.9\linewidth]{imgs/threshold}
		\caption{Threshold filter}
		\label{fig:threshold}
	\end{subfigure}  
	\caption{The color and depth images captured by the \mbox{cameras} and the filtered depth image.}
	\label{fig:rgbd-data}
\end{figure}
%------------------------------------------------------------------------
\section{Reconstruction Pipeline}
\label{sec:reconstruction-pipeline}
To perform the real time 3D reconstruction we implemented the pipeline which as is displayed in \fref{fig:reconstruction-pipeline} and described in the following section.\\
All tests were performed on a Laptop with an Intel\textsuperscript{\textregistered} Core\texttrademark{} \mbox{i7-6700HQ} CPU @ 2.60GHz~\cite{CPU}, 40GB of DDR4 RAM and a Nvidia GeForce GTX 970M~\cite{GTX970}.

%-------------------------------------------------------------------------
\subsection{RGB-D Frame Capture}
We use three \rscamera{} for our capture setup which work on the concept of active stereo depth. Two cameras are placed on the sides of the camera and the depth is calculated from a triangulation using the displacement. An infrared pattern is projected into the scene to enhance the details.\\
We process the depth images using a threshold filter which discards pixels that are outside of a certain distance range during runtime. We also tried different hole-filling, spatial and edge enhancing filters that did not improve the results and thus were not used in the final implementation.\\
\fref{fig:rgbd-data} displays the raw RGB-D images and the filtered depth image.

We tried to extend the three camera setup with a fourth one but were not able to integrate it due to USB bandwidth limitations. Furthermore, we had to limit the color and depth resolutions as well as the framerate for the cameras to allow processing of the input data in real time. The resolution we used for the color and depth stream is 848x480~px at 30 frames per second. For further insight and details about the limitations of the cameras in a multi camera setup we refer you to \cite{RealSenseWhitepaper}.
\newpage
The RGB-D images from the three cameras are time synchronized~\cite{RealSenseWhitepaper} what improves the results of the following camera calibration and the final mesh reconstruction.
The synchronization is carried out through a hardware cable between the cameras~\cite{RealSenseWhitepaper}. This improves the results of the camera calibration and the final mesh reconstruction.

\intel{} provides an excellent SDK called \mbox{RealSenseSDK~\cite{RealSenseSDK}}, an extensive documentation and updates for its API which made it easy to use the cameras.

%------------------------------------------------------------------were-------
\subsection{Correspondence Finding}
We use \aruco{} to find correspondences between the three camera streams. These markers have a unique identifier and can easily be detected in every color image using the \opencv{} library.\\
The markers are mounted on a wooden cube box as shown in \fref{fig:cube}.

By placing the cameras such that each is looking towards one of the cube's corners, we can assure that all three markers are visible in every color image. This also ensures that between every pair of cameras we have an overlap of two markers for which the eight corner positions are detected sub pixel perfect. This gives us a total of $8*3=24$ constraints between every camera pair which are used in the next step to estimate the relative transformation between the markers.

It is also important to mention that we tried \charuco{} which are a mixture of \aruco{} and a chessboard pattern which allow simultaneous detection and pose estimation. However, the \charuco{} did not give us good pose results and slowed down the calculation (\tref{tab:camera-calibration}). It also reduced the range of the cameras where the markers could be detected so we stick to \aruco{}. 
%
%
\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.6\linewidth]{imgs/cube}
	\end{center}
	\caption{Marker cube with the detected markers highlighted (green) and their unique identifiers (red).}
	\label{fig:cube}
\end{figure}
%
%
%-------------------------------------------------------------------------
\subsection{Camera Calibration}
We use the known marker positions in the color image and combine them with the depth information to calculate the 3D position of the marker corners. As we ensure to have an overlap of two markers between every pair of cameras we now have $8*3=24$ constraints (8 markers each with a x, y and z value). The 3D positions are then used to find the relative transformations between the cameras and to align the pointclouds accordingly.
%
%-------------------------------------------------------------------------
\subsubsection{Procrustes}
The Procrustes algorithm aligns two pointclouds using known correspondences by estimating the relative translation and rotation between two sets of points.
\paragraph{Translation}
We calculate the center of gravity for every pointcloud by summing up the individual point positions and dividing by the number of points. This gives us for every set of points the mean point. The translation is then estimated by the difference between the centers of gravity.
\begin{equation}
	T_{ij} = C_i - C_j
\end{equation}
\paragraph{Rotation}
The rotation between two pointclouds can be calculated by optimizing over the unknown rotation $R_{ij}$. This is done by minimizing the mean squared error between the set of points $X_i$ and the $X_j$.
\begin{equation}
	\min_{R_{ij}} || X_i - R_{ij} X_j ||^2_2
\end{equation}
Fortunately there exists a closed form solution for the rotation $R_{ij}$ which is based on the singular value decomposition of the matrix $X_i^T X_j$.
\begin{align}
	X_i^T X_j &= U \Sigma V^T \\
	R_{ij} &= UV^T
\end{align}
\tref{tab:camera-calibration} displays the duration and mean squared error values after the Procrustes alignment. At this point the error is still fairly high, so we had to further align the pointclouds.
%
\subsubsection{Point-to-Point Error}
In order to improve the alignment which we get from Procrustes we use a Point-to-Point correspondence error. We optimize the relative translation $T_{ij}$ and rotation $R_{ij}$ of every pair of camera frames $i$ and $j$ and in each of these for the known correspondences $k$, resulting in the following energy term:
\begin{equation}\label{eq5}
\min_{T_{ij}, R_{ij}} \sum_{i}^{frame}\sum_{j}^{frame}\sum_{k}^{corres.} || X_{ik} - T_{ij}R_{ij}*X_{jk} ||_2^2
\end{equation}
A comparison of the results from this optimization can be seen in \tref{tab:camera-calibration}. It shows that the mean squared error is much lower using the transformation from the Point-to-Point optimization. The increase in the duration of the estimation is due to the algorithm's iterative nature but as the pose estimation is done only once in the beginning the increase is negligible.
%
%-------------------------------------------------------------------------
\subsection{Voxelgrid TSDF}
We calculate a truncated signed distance field (TSDF) based on a voxelgrid to represent the implicit surface of the aligned pointclouds. The voxelgrid is a three-dimensional cuboid structure of a given resolution where the grid is filled with points (voxels). An exemplary voxelgrid is displayed in \fref{fig:voxelgrid}. 

We fuse the aligned pointclouds into the voxelgrid by iterating over every voxel and projecting it into the depth frame of the camera. We then lookup the surface depth value $z_{depth}$ at the projected position in the depth image and compare it to the depth of the voxel. This gives us the distance $d_i$ of the voxel to the surface in the $i$-th camera frame:
\begin{equation}\label{eq1}
d_i = z_{voxel} - z_{depth,i}
\end{equation}
The final TSDF value is then calculated by truncating the distance with a given threshold $t$:
\begin{equation}
	tsdf_i = min(max(-t, d_i), t)
\end{equation}
We do this integration for all three sets of points coming from the cameras. To get a valid representation of the implicit surface for all pointclouds we perform a weighted averaging: 
\begin{equation}\label{eq2}
tsdf_{i+1}=\frac{tsdf_{i} * weight}{weight+1}
\end{equation}

As we compute the TSDF values for a large number of voxels we decided to parallelize the calculations on the GPU~(\sref{sec:reconstruction-pipeline}) to increase the performance through hardware accelerated parallelization. Since we are using \opengl{} for the rendering we decided to also use \opengl{} compute shaders~\cite{Authors1}. \tref{tab:voxelgrid} displays the duration for integrating all three camera frames into the voxelgrid at varying resolutions and their number of voxels. \\

\begin{figure}[t]
\begin{center}
\includegraphics[width=0.6\linewidth]{imgs/tsdf}
\end{center}
 \caption{A two-dimensional voxelgrid with an implicit surface (red) and the respective TSDF values. The TSDF values are positive outside of the surface and negative inside. The zero-crossing represents the surface.}
 \label{fig:voxelgrid}
\end{figure}

%-------------------------------------------------------------------------
\subsection{Marching Cubes Surface Extraction}
We extract the implicit surface represented by the voxelgrid using the Marching Cubes algorithm~\cite{MarchingCubes}.\\
We iterate over all voxelcells that are spanned up by eight adjacent voxels in the voxelgrid. Within these cells the TSDF values for all eight corners are looked up in the voxelgrid and then used to find the zero-crossings along the edges between every pair of corners. We use these zero-crossings to look up the triangulation for the voxelcell based on the Marching Cubes tables described by \cite{MarchingCubes}. This gives us the triangulated mesh as a final output that approximates the implicit surface represented by the voxelgrid. \fref{fig:wireframe} shows a triangulation of one of our group members at a resolution of 10mm.

We implemented a two pass \opengl{} compute shader to calculate the triangulation. The upper bound for the number of triangles that can be generated during Marching Cubes is 5 times the number of voxelcells. This gives a possible count of $(200 * 200 * 200) * 5 = 40.000.000$ triangles for e.g. a voxelgrid of $200^3$ voxels. In reality, the numbers of actually generated triangles is much smaller with $200.000 - 250.000$ which displays the need of a two pass shader. The first pass counts the number of triangles that will be generated and allocates memory. In the second pass we then generate the triangles and fill the memory that exactly fits the needs without overhead.

\tref{tab:marching-cubes} displays the number of triangles that are generated by the algorithm at different resolutions and the respective durations. It can be seen that down to a 5mm resolution we are easily real time capable at 60 frames per second.

%------------------------------------------------------------------------
\section{Results and Conclusion}
\sref{sec:images} shows a variety of reconstructed scenes using our reconstruction pipeline.

We have achieved our goal to get a real time mesh reconstruction of dynamic scenes using a multi view camera capture setup. We can calculate the relative poses between the cameras during calibration to align the pointclouds that the three cameras capture. By fusing these sets of points into a voxelgrid we can represent the implicit surface and extract it using the Marching Cubes algorithm. All these steps are performed on GPU which gives us a real time reconstruction of dynamic scenes.

%------------------------------------------------------------------------
\section{Future Work}
Since we optimize the relative transformations during camera calibration for a small number of markers (correspondences), a further improvement could be achieved by implementing a Nearest Neighbor Search. This would result in a much higher number of correspondences on which global alignment techniques like Iterative Closest Points (ICP) can be performed.

Furthermore, we currently see a high number of artifacts that are generated at the overlap of the three camera streams (\fref{fig:reconstruction-all-cameras}). To reduce these a next step could be to calculate normal maps for every depth stream and to use these to improve the weighted averaging during TSDF calculation.

Finally, in order to improve the whole reconstruction, the reconstructed scene could be tracked over time. By using the temporal information and the deformations between the frames a canonical model could be generated. This model allows to update only the parts that changed as well as reduce artifacts which would improve the reconstruction and the performance further.

%------------------------------------------------------------------------
\section{Group Member Contributions}
\paragraph{Research}
This includes the gathering of information as well as trying different approaches for the implementation. The contributions are ordered descending:\\
Marcel Bruckner, Kevin Bein, Moiz Sajid

\paragraph{Final Presentation}
The final presentation was prepared by Kevin Bein, extended by Marcel Bruckner and finalized as a group.

\paragraph{Final Report}
The \LaTeX{} setup for the final report and the first draft was done by Moiz Sajid. The final version was extended by Marcel Bruckner and proofread by Kevin Bein.

\subsection{Code}
The code, as it is by the end of this project, was written by Marcel Bruckner.
% Much time was also spent by Kevin Bein trying different approaches but didn't make it in the final version of the project. 
\paragraph{RGB-D Frame Capture}
The capture classes and frame processing were tested by Marcel Bruckner and Kevin Bein and finally implemented by Marcel Bruckner.\\
Hardware synchronization was implemented by Kevin Bein.\\
The depth map filtering was done by Marcel Bruckner and Kevin Bein.\\
Furthermore, we spent many hours as a group in the beginning of the project to get to used to the RealSense SDK. 

\paragraph{Correspondence Finding}
The correspondence finding as it is now was done by Marcel Bruckner. This includes the printing of the markers, the assembling of the marker cube and the code needed to detect the markers. Also much of the now not used ChArUco code was done by him.\\
Kevin Bein tried many different approaches using the \charuco{} and explored the possibilites of OpenCV for usage in the project. He also helped with cutting out the wood for the cube.

\paragraph{Camera Calibration}
The camera calibration was done by Marcel Bruckner. Procrustes and the Point-to-Point error function was implemented by him. Finding out the correct calibration parameters and setting up the cameras is contributed to him.\\
Kevin Bein worked on aligning correspondences via the calculated rotation/translations matrices and correcting and extending the initial rendering procedure to display the results. \\
Moiz Sajid tried out chessboard pattern for calibrating two cameras which never made it to the final project because it was not feasible. Moiz Sajid worked on ICP using Marcel's correspondence finding code which never made it to the project since it essentially did the same what Marcel's existing implementation did (discovered late). ICP should have been implemented alongside FLANN however this essential use case was discovered very late.   

\paragraph{Voxelgrid TSDF}
The voxelgrid and TSDF calculation (CPU and GPU) was entirely implemented by Marcel Bruckner.\\
Moiz Sajid utilized the existing implementation of tsdf fusion using C++~\cite{Authors2} and Python~\cite{Authors8} which worked well but only for uncalibrated cameras hence was not used in the project. It could have been utilized alongside using other techniques for alignment. \\
Kevin Bein worked with the same existing tsdf implementation \cite{Authors2} early on but gave away this part of the project to Marcel Bruckner and Moiz Sajid to focus on Rendering.

\paragraph{Marching Cubes Surface Extraction}
The Marching Cubes surface extraction (CPU and GPU) was entirely implemented by Marcel Bruckner.\\
Kevin Bein implemented MC using the Point Cloud Library (PCL) to shorten development time but later disregarded the attempt due to the installation complexity of PCL and Marcel Bruckner finishig his native implementation earlier. \\
Moiz Sajid worked on a working Marching Cubes CPU implementation which never made it to the final project because Marcel's Marching Cubes implementation was used instead for no particular reason. 

\paragraph{Rendering}
The rendering was entirely implemented by Marcel Bruckner.\\
Kevin Bein early on worked a lot on tweaking the provided rendering routines from RealSense but after little success and a group decision to move on to a custom implementation experimented a lot with OpenGL shaders. A Phong lightning and voxelgrid shader rendering never made it into the final code. \\
Moiz Sajid worked on Aljaz's code for point cloud rendering which never made it to the final project. Moiz Sajid also wrote shaders for displaying single frame mesh which again never made it to the final project. 

\paragraph{Compute Shaders}
The compute shaders were entirely implemented by Marcel Bruckner.

\paragraph{Point Cloud Library (Not used in the project)}
Kevin Bein spent a lot of time utilizing many parts of PCL for the project. He implemented working solutions for MC and tried out different other functionality. The fact that compiling PCL with GPU support did not work and the overal installation complexity of PCL plus the fact that, meanwhile, Marcel Bruckner succeeded in creating a working, custom implementation, ultimately lead to abandon the idea of further using PCL. \\
Moiz Sajid spent some time in building PCL and FLANN with Nvidia Cuda Support from source to see if it is feasible for the project.

\newpage
\section{Tables}
\begin{table}[h!]
  \begin{center}
    \begin{tabular}{c|c|c|c}
      \textbf{Algorithm} & \textbf{Iter.} & \textbf{MSE} & \textbf{Time [ms]}\\
      \hline
      Procrustes (A)  & 1  & \textless 0.7 - 1.73 & 8 - 20\\
      Point-to-Point (A) & 1 - 20  & \textless 0.1 & 900 - 1000\\
      Procrustes (C) & 1  & 0.25 - 1.29 & 20 - 40\\
      Point-to-Point (C) & 1 - 20  & \textless 0.1 & $\pm$ 20.000\\
    \end{tabular}
     \caption{Comparisons between different optimization algorithms in terms of iterations, MSE (Mean Squared Error) and time. A = ArUco, C = ChArUco}
     \label{tab:camera-calibration}
  \end{center}
\end{table}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{c|c|c|c}
      \textbf{Algorithm} & \textbf{Res. [mm]} & \textbf{Voxels} & \textbf{Time [ms]}\\
      \hline
      Voxelgrid & 20 & 125.000 & \textless 1\\
      Voxelgrid & 10 & 1.000.000 & \textless 1\\
      Voxelgrid & 5 & 8.000.000 & 5\\
      Voxelgrid & 4 & 15.625.000 & 20\\
      Voxelgrid & 3 & 37.000.000 & 50\\
    \end{tabular}
     \caption{Comparison of the time needed to calculate the TSDF values in a $1m^3$ voxelgrid for a multitude of voxelgrid resolution.}
     \label{tab:voxelgrid}
  \end{center}
\end{table}

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{c|c|c|c}
      \textbf{Algorithm} & \textbf{Res. [mm]} & \textbf{Triang. [$\pm 20\%$]}  & \textbf{Time [ms]}\\
      \hline
      March. Cubes & 20  & 10.000   & \textless 1\\
      March. Cubes & 10  & 40.000  & \textless 1\\
      March. Cubes & 5 & 200.000   & 15\\
      March. Cubes & 4 & 400.000   & 35\\
      March. Cubes & 3 & 800.000    & 80\\
    \end{tabular}
     \caption{Comparison of the time needed to calculate the triangulation of the implicit surface represented by a $1m^3$ voxelgrid using the Marching Cubes algorithm at varying resolutions.}
     \label{tab:marching-cubes}
  \end{center}
\end{table}

\clearpage
\section{Images}
\label{sec:images}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{imgs/res2}
	\end{center}
	\caption{Triangulated mesh of one of our group members using one camera. $1m^3$ voxelgrid at 10 mm resolution.}
	\label{fig:wireframe}
\end{figure}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{imgs/res3}
	\end{center}
	\caption{Reconstructed surface of one of our group members using all three cameras. The artifacts that arise in the parts where the pointclouds of the cameras overlap (face) can be seen. $1m^3$ voxelgrid at 5 mm resolution.}
	\label{fig:reconstruction-all-cameras}
\end{figure}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{imgs/cube_reconstructed.png}
	\end{center}
	\caption{Reconstructed surface of the marker cube using all three cameras. The artifacts that arise in the parts where the pointclouds of the cameras overlap (cube edges) can be seen. $1m^3$ voxelgrid at 5 mm resolution.}
	\label{fig:scene-1}
\end{figure}

\begin{figure}[h!]
	\begin{center}
		\includegraphics[width=0.9\linewidth]{imgs/scene.png}
	\end{center}
	\caption{Reconstructed surface of an arbitrary scene using all three cameras. The ChArUco board (middle), a hole puncher (blue), a stapler (lower right) and a DSLR camera (top left) are displayed. $1m^3$ voxelgrid at 5 mm resolution.}
	\label{fig:scene-2}
\end{figure}

\clearpage

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
